<?xml version="1.0" encoding="UTF-8" ?><rss version="2.0"><channel><title>Data Science Challenge / Competition</title><link>https://iphysresearch.github.io/DataSciComp</link><description>Latest update at 05/16/2019 (GMT+0800).</description><item><title>2019世界机器人大赛—BCI脑控机器人大赛 暨 第三届中国脑机接口比赛</title><link>http://www.worldrobotconference.com/html/jiqirendasai/saishi/2018/0314/290.html</link><category>PF</category><category>RL</category><pubDate>Thu, 16 May 2019</pubDate><description>脑-机接口（BCI）是指通过对神经系统电活动和特征信号的收集、识别及转化，使人脑发出的指令能够直接传递给指定的机器终端，从而使人对机器人的控制和操作更为高效便捷，该项技术在人与机器人的交流沟通领域有着重大创新意义和使用价值，其已广泛应用于助残康复、灾害救援、娱乐体验等多个领域，并在改善残疾人生活质量中做出了巨大贡献。</description></item><item><title>Game of Deep Learning: Computer Vision Hackathon</title><link>https://datahack.analyticsvidhya.com/contest/game-of-deep-learning/</link><category>PF</category><category>CV</category><pubDate>Thu, 16 May 2019</pubDate><description>Do you feel passionate about solving problems through deep learning? Do you aspire to take data science to millions of people out there? Can the leader in you make people follow data science out of sheer passion? If the answer to all the questions is yes – look no more. Analytics Vidhya is looking for evangelists who can carry and deliver their baton to the world.</description></item><item><title>Learning to Run a Power Network (L2RPN 2019)</title><link>https://l2rpn.chalearn.org</link><category>PF/AC</category><category>RL</category><pubDate>Thu, 16 May 2019</pubDate><description>The challenge is based on an environment (and not solely a dataset) in which an agent can learn by interactions. </description></item><item><title>Data Science for Good: City of Los Angeles</title><link>https://www.kaggle.com/c/data-science-for-good-city-of-los-angeles</link><category>PF</category><category>DM</category><pubDate>Thu, 16 May 2019</pubDate><description>Help the City of Los Angeles to structure and analyze its job descriptions. The goal is to convert a folder full of plain-text job postings into a single structured CSV file and then to use this data to: (1) identify language that can negatively bias the pool of applicants; (2) improve the diversity and quality of the applicant pool; and/or (3) make it easier to determine which promotions are available to employees in each job class.</description></item><item><title>SMP 2019 ETST 第二届文本溯源技术评测</title><link>https://www.biendata.com/competition/smpetst2019/</link><category>PF</category><category>NLP</category><pubDate>Thu, 16 May 2019</pubDate><description>SMP 2019文本溯源评测由中国中文信息学会社会媒体处理专业委员会主办，黑龙江工程学院承办。本次技术评测以科研立项或成果创新型审查为应用背景，文本溯源的目标是判断一个文本的内容是否复制或改编于另外一个或者多个文本。文本溯源技术在学术诚信检测、搜索引擎优化等领域有广泛应用。</description></item><item><title>WIDER Face &amp; Person Challenge 2019</title><link>http://wider-challenge.org/2019.html</link><category>PF/AC</category><category>CV/NLP</category><pubDate>Thu, 16 May 2019</pubDate><description>Following the success of the <a href="http://wider-challenge.org/2018.html">First WIDER Challenge Workshop</a>, we organize a new round of challenge in conjunction with <a href="http://iccv2019.thecvf.com">ICCV 2019</a>. The challenge centers around the problem of precise localization of human faces and bodies, and accurate association of identities. It comprises of four tracks: <a href="http://wider-challenge.org/comingsoon.html">WIDER Face Detection</a>, aims at soliciting new approaches to advance the state-of- the-art in face detection. <a href="https://competitions.codalab.org/competitions/22852">WIDER Pedestrian Detection</a>, has the goal of gathering effective and efficient approaches to address the problem of pedestrian detection in unconstrained environments. <a href="https://competitions.codalab.org/competitions/22833">WIDER Cast Search by Portrait</a>, presents an exciting challenge of searching cast across hundreds of movies. <a href="https://competitions.codalab.org/competitions/22864">WIDER Person Search by Language</a>, aims to seek new approaches to search person by natural language.</description></item><item><title>Northeastern SMILE Lab - Recognizing Faces in the Wild</title><link>https://www.kaggle.com/c/recognizing-faces-in-the-wild/overview/description</link><category>PF</category><category>CV</category><pubDate>Thu, 16 May 2019</pubDate><description>Can you determine if two individuals are related?</description></item><item><title>Graph Golf: The Order/degree Problem Competition</title><link>http://research.nii.ac.jp/graphgolf/</link><category>PF/AC</category><category>DM/CV</category><pubDate>Thu, 16 May 2019</pubDate><description><b>Find a graph that has smallest diameter &amp; average shortest path length given an order and a degree.</b> Graph Golf is an international competition of the order/degree problem since 2015. It is conducted with the goal of making a catalog of smallest-diameter graphs for every order/degree pair. Anyone in the world can take part in the competition by submitting a graph. Outstanding authors are awarded in CANDAR 2019, an international conference held in Nagasaki, Japan, in November 2019.</description></item><item><title>Propensity to Fund Mortgages</title><link>https://www.crowdanalytix.com/contests/propensity-to-fund-mortgages</link><category>PF/AC</category><category>DM</category><pubDate>Thu, 16 May 2019</pubDate><description>Develop a model to predict, given mortgage application information, whether the mortgage will be funded or not. To predict whether a mortgage will be funded using only this application data, certain leading factors driving the loan’s ultimate status will be identified. Solvers will discover the specific aspects of the dataset that have the greatest impact, and build a model based on this information.</description></item><item><title>Identify Characters from Product Images</title><link>https://www.crowdanalytix.com/contests/identify-characters-from-product-images</link><category>PF/AC</category><category>CV</category><pubDate>Thu, 16 May 2019</pubDate><description>Identify the characters from product image from a list of 42 possible values. While using machine learning to perform image recognition is currently one of the most popular use cases, in some cases, the existing large-scale models are too broad to be effective for specific business use cases. In this contest we will use a data driven approach to identify the “characters” in an image (product images). </description></item><item><title>nuScenes detection challenge</title><link>https://www.nuscenes.org/</link><category>PF/AC</category><category>CV</category><pubDate>Thu, 16 May 2019</pubDate><description>The nuScenes dataset is a large-scale autonomous driving dataset.</description></item><item><title>ActivityNet-Entities Object Localization Task</title><link>https://github.com/facebookresearch/ActivityNet-Entities</link><category>PF/AC</category><category>CV/NLP</category><pubDate>Thu, 16 May 2019</pubDate><description>ActivityNet-Entities, is based on the video description dataset ActivityNet Captions and augments it with 158k bounding box annotations, each grounding a noun phrase (NP). Here we release the complete set of NP-based annotations as well as the pre-processed object-based annotations. please see our dataset <a href="https://github.com/facebookresearch/ActivityNet-Entities">repo</a>, code <a href="https://github.com/facebookresearch/grounded-video-description">repo</a>, and CVPR 2019 oral <a href="https://arxiv.org/pdf/1812.06587.pdf">paper</a>.</description></item><item><title>YouCook2 Dense Video Captioning</title><link>http://youcook2.eecs.umich.edu</link><category>PF/AC</category><category>CV/NLP</category><pubDate>Thu, 16 May 2019</pubDate><description>YouCook2 is currently suitable for video-language research, weakly-supervised activity and object recognition in video, common object and action discovery across videos and procedure learning.</description></item><item><title>Objects 365 图片物体检测</title><link>https://www.biendata.com/competition/objects365/</link><category>PF</category><category>CV</category><pubDate>Mon, 06 May 2019</pubDate><description>Objects365是一个全新的数据集，旨在促进对自然场景不同对象的检测研究。Objects365在638K张图像上标注了365个对象类，训练集中共有超过1000万个边界框。因此，这些标注涵盖了发生在各种场景类别中的常见对象。</description></item><item><title>Crowd Human 人体检测</title><link>https://www.biendata.com/competition/crowdhuman/</link><category>PF</category><category>CV</category><pubDate>Mon, 06 May 2019</pubDate><description>CrowdHuman是目前最大的人体检测数据集，带有充分的标注信息，图片来源具有丰富的多样性。包含15000张训练集，4370张验证集和5000张测试集，在数据集中，总共有340K个人体目标，平均每张图片有22.6个人，数据集中有各种各样的遮挡场景。每个人的实例都有一个头部边界框、可见区域边界框和全身边界框。</description></item><item><title>2019中国高校计算机大赛——大数据挑战赛</title><link>https://www.kesci.com/home/competition/5cb80fd312c371002b12355f</link><category>PF</category><category>NLP</category><pubDate>Mon, 06 May 2019</pubDate><description>2019大数据挑战赛（以下简称“大赛”）是在中国高校计算机大赛主办单位的指导下，由清华大学、南开大学与字节跳动公司联合主办，亚马逊AWS提供资源支持以及科赛提供竞赛平台支持，并以企业真实场景和实际数据为基础的高端算法竞赛。大赛面向全球高校在校生开放，旨在提升高校学生对数据分析与处理的算法研究与技术应用能力，探索大数据的核心科学与技术问题，尝试创新大数据技术，推动大数据的产学研用，本次大赛鼓励高校教师参与指导。 预选赛——文本情感分类模型 正式赛题——文本点击率预估（5月26日开赛）</description></item><item><title>CCKS 2019 中文知识图谱问答</title><link>https://www.biendata.com/competition/ccks_2019_6/</link><category>PF/AC</category><category>NLP</category><pubDate>Mon, 06 May 2019</pubDate><description>本评测任务为基于中文知识图谱的自然语言问答，简称CKBQA （Chinese Knowledge Base Question Answering）。即输入一句中文问题，问答系统从给定知识库中选择若干实体或属性值作为该问题的答案。问题均为客观事实型，不包含主观因素。理解并回答问题的过程中可能需要进行实体识别、关系抽取、语义解析等子任务。这些子任务的训练可以使用额外的资源，但是最终的答案必须来自给定的知识库。</description></item><item><title>CCKS 2019 面向金融领域的事件主体抽取</title><link>https://www.biendata.com/competition/ccks_2019_4/</link><category>PF/AC</category><category>NLP</category><pubDate>Mon, 06 May 2019</pubDate><description>本次评测任务的主要目标是从真实的新闻语料中，抽取特定事件类型的主体。即给定一段文本T，和文本所属的事件类型S，从文本T中抽取指定事件类型S的事件主体。</description></item><item><title>OpenEDS Challenge</title><link>https://research.fb.com/programs/openeds-challenge</link><category>PF/AC</category><category>CV</category><pubDate>Mon, 06 May 2019</pubDate><description>In the absence of accurate gaze labels, we propose to advance the state of the art by carefully designing two challenges that combine human annotation of eye features with unlabeled data. These challenges focus on deeper understanding of the distribution underlying human eye state. We invite ML and CV researchers for participation. <a href="https://evalai.cloudcv.org/web/challenges/challenge-page/353">Track-1 Semantic Segmentation challenge</a> <a href="https://evalai.cloudcv.org/web/challenges/challenge-page/354">Track-2 Synthetic Eye Generation challenge</a></description></item><item><title>nocaps</title><link>https://nocaps.org</link><category>PF/AC</category><category>NLP</category><pubDate>Mon, 06 May 2019</pubDate><description>Image captioning models have achieved impressive results on datasets containing limited visual concepts and large amounts of paired image-caption training data. However, if these models are to ever function in the wild, a much larger variety of visual concepts must be learned, ideally from less supervision. To encourage the development of image captioning models that can learn visual concepts from alternative data sources, such as object detection datasets, we present the first large-scale benchmark for this task. Dubbed nocaps, for novel object captioning at scale, our benchmark consists of 166,100 human-generated captions describing 15,100 images from the Open Images validation and test sets. The associated training data consists of COCO image-caption pairs, plus Open Images imagelevel labels and object bounding boxes. Since Open Images contains many more classes than COCO, nearly 400 object classes seen in test images have no or very few associated training captions (hence, nocaps). We extend existing novel object captioning models to establish strong baselines for this benchmark and provide analysis to guide future work on this task.</description></item><item><title>PatchCamelyon</title><link>https://github.com/basveeling/pcam</link><category>PF/AC</category><category>CV</category><pubDate>Mon, 06 May 2019</pubDate><description>The PatchCamelyon benchmark is a new and challenging image classification dataset. It consists of 327.680 color images (96 x 96px) extracted from histopathologic scans of lymph node sections. Each image is annoted with a binary label indicating presence of metastatic tissue. PCam provides a new benchmark for machine learning models: bigger than CIFAR10, smaller than imagenet, trainable on a single GPU.</description></item></channel></rss>